\section{Related Work}
\label{sec:relatedWork}



\subsection{How to control mean}

In our previous work, we proved that we can easily control mean position of the swarm. We control mean position with a PD controller that uses the mean position and mean velocity. Our control input is the global force applied to each robot:
\begin{align}
u_x &= K_{p}(x_{goal} - \bar{x}) + K_{d}(0-\bar{v}_x) \nonumber\\
u_y &= K_{p}(y_{goal}  - \bar{y}) + K_{d}(0-\bar{v}_y)  \label{eq:PDcontrolPosition}
\end{align}
here $K_{p}$ is the proportional gain, and $K_{d}$ is the derivative gain. We performed a parameter sweep to identify the best values.  Representative experiments are shown in Fig.~\ref{fig:gainvalues}. 100 robots were used and the maximum speed was 3 meters per second. As shown in Fig.~\ref{fig:gainvalues}, we can achieve an overshoot of 1\% and a  rise time of 1.52 s with $K_{p}= 4$, and  $K_{d} = 1$. 

\begin{figure}
\centering
\begin{overpic}[width = \columnwidth]{gainvalues.eps}
\end{overpic}
\vspace{-1em}
\caption{\label{fig:gainvalues} Tuning proportional ($K_p$, top) and derivative ($K_d$, bottom)  gain values in \eqref{eq:PDcontrolPosition} improves performance. These plots show convergence with 100 robots.
%\vspace{-2em}
}
\end{figure}

\subsection{How to control variance}

\begin{figure}
\centering
\begin{overpic}[width = \columnwidth] {brownianfig.eps}
\end{overpic}
\vspace{-1em}
\caption{\label{fig:varyBrownian} Increased noise results in more responsive variance control because stronger Brownian noise causes a faster increase of variance.
%\vspace{-2em}
}
\end{figure}

For variance control we move away from the wall and wait to increase variance because Brownian noise naturally disperses the swarm in such a way that the variance increases linearly.  If faster dispersion is needed, the swarm can be pushed through obstacles such as a diffraction grating or Pachinko board. %no need to worry about blocks in this controller.

The variance control law to regulate the variance to $\sigma^2_{ref}$ has three gains:
\begin{align}
u_x &= K_{p}(x_{goal}(\sigma^2_{ref}) - \bar{x}) - K_{d}\bar{v}_x + K_{i}(\sigma^2_{ref}-\sigma^2_{x}) \nonumber\\
u_y &= K_{p}(y_{goal}(\sigma^2_{ref})  - \bar{y}) - K_{d}\bar{v}_y + K_{i}(\sigma^2_{ref}-\sigma^2_{y}).  \label{eq:PDcontrolVariance}
\end{align}
In a slight abuse of notation we call the gain scaling the variance error $K_i$ because the variance integrates over time.
Eq.~\ref{eq:PDcontrolVariance} assumes the nearest wall is to the left of the robot at $x=0$, and chooses a reference goal position that in steady-state would have the correct variance:
\begin{align}
x_{goal}(\sigma^2_{ref}) = r + \sqrt{3\sigma^2_{ref}}
\end{align}
 If another wall is closer, the signs of $[K_p,K_i]$ are inverted, and the location $x_{goal}$ is translated.  Results are shown in Fig.~\ref{fig:varyBrownian}, with $K_{p,i,d} = [4,1,1]$.

